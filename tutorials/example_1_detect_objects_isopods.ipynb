{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Measuring shape, size and colour of isopods  \n",
    "\n",
    "Little nugget of information: this was the original computer vision problem that phenopype was intended to solve (it's predecessor [\"iso_cv\"](https://github.com/mluerig/iso_cv) was not much more than a script-collection).\n",
    "\n",
    "In this example I am demonstrating all three phenopype workflows ([prototyping](#Prototyping), [low thoughput](#Low-throughput) and [high throughput](#High-throughput)). For a project, you would probably only use either the high or low throughput approach, but the protoypting approach for this example decomposes the all the required steps for a classic computer vision workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row; text-align: left\">\n",
    "    \n",
    "<div class=\"col-md-6\">\n",
    "    \n",
    "![Before](_assets/ex1_before.jpg)\n",
    "    \n",
    "**Input** - Freshwater isopod, alive, photographed on a white resin-tray from a camera stand. \n",
    "</div>\n",
    "<div class=\"col-md-6\">\n",
    "\n",
    "![After](_assets/ex1_after.jpg)\n",
    "    \n",
    "**Results** - Isopod shape, size and colour are extracted (and size referenced using the reference card) \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the image\n",
    "\n",
    "First we import the image from the a filepath using `load_image`. With the flag `df=True` we can extract some image-meta information that we want to be represented in the results files later (i.e. image name and its dimensions). After loading it, we can have a quick look at it with the `show_image` function - you can close it again with Enter or Esc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phenopype as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"images/isopods.jpg\"\n",
    "image, img_data = pp.load_image(filepath, df=True)\n",
    "pp.show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>size_ratio_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isopods.jpg</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename  width  height  size_ratio_original\n",
       "0  isopods.jpg   2100    1400                    1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data ## size ratio refers whether this image has been resized using the resize argument of `load_image`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing a mask\n",
    "\n",
    "The original image has a lot of noise, e.g. the non-white area around the tray, water reflections, the label, the reference card, and the little fecal pellets that lie around on the tray. Classic computer vision algorithms are unspecific to the object, so they will pick up any object that is darker than its environment. Therefore, a useful preprocessing step is to exclude some of that noise by applying a mask. With the `create_mask` function, you can include or exclude certain areas of the image from the following analysis steps (`include=True/False`). \n",
    "\n",
    "Here, we want to include all of the wite tray that has isopods in it: Hold the left button down and drag the rectangle shaped mask tool over the area you want to include (for more details on the mask and other interaction-tools, see [Tutorial 5](tutorial_5_gui_interactions.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- create mask\n"
     ]
    }
   ],
   "source": [
    "masks = pp.preprocessing.create_mask(image,df_image_data=img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create size reference\n",
    "\n",
    "Within the area we masked lies the reference card. We want to include its information in our results files (the pixel-to-mm-ratio), so we supply the image meta DataFrame with `df_image_data=img_data`. However, we do not want the card itself to be detected, so we mask it with the argument `mask=True` and by supplying or mask DataFrame with `df_masks=masks` to have both masks in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style=\"width:500px; text-align: left\">\n",
    "    \n",
    "![Adding a scale](_assets/ex1_scale.gif)\n",
    "    \n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- measure pixel-to-mm-ratio\n",
      "Scale set\n",
      "- add column length\n",
      "Template selected\n"
     ]
    }
   ],
   "source": [
    "img_data, masks = pp.preprocessing.create_scale(image, \n",
    "                                                mask=True,\n",
    "                                                df_image_data=img_data, \n",
    "                                                df_masks=masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `masks` DataFrame contains two masks: the one we created above that *includes* the tray and the one we made to *exclude* the scale reference card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>size_ratio_original</th>\n",
       "      <th>mask</th>\n",
       "      <th>include</th>\n",
       "      <th>coords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isopods.jpg</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>mask1</td>\n",
       "      <td>True</td>\n",
       "      <td>[(249, 180), (1898, 180), (1898, 1353), (249, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isopods.jpg</td>\n",
       "      <td>2100</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>scale</td>\n",
       "      <td>False</td>\n",
       "      <td>[(441, 1017), (682, 1017), (682, 1233), (441, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename  width  height  size_ratio_original   mask include  \\\n",
       "0  isopods.jpg   2100    1400                    1  mask1    True   \n",
       "0  isopods.jpg   2100    1400                    1  scale   False   \n",
       "\n",
       "                                              coords  \n",
       "0  [(249, 180), (1898, 180), (1898, 1353), (249, ...  \n",
       "0  [(441, 1017), (682, 1017), (682, 1233), (441, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a quick look at both masks together by calling the visualization function `show_mask`, followed by `show_image`. For this we should create a new array and *not* overwrite the original image loaded before - here we call it `canvas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - show mask: mask1.\n",
      " - show mask: scale.\n"
     ]
    }
   ],
   "source": [
    "canvas = pp.visualization.show_masks(image, df_masks=masks, line_width=3)\n",
    "pp.show_image(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - 1st attempt\n",
    "\n",
    "Now that we have removed most of the noise, we can implement an algorithm that segments the imgae into foreground and background (check the [resources section](resources.html#computer-vision) of the documentation. Here we use the `threshold` algorithm that to detect the isopods as foreground and the tray as background. By supplying the mask DataFrame we created before with `df_masks=masks`, we can exclude the unwanted regions of the image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- applying mask: mask1\n",
      "- applying mask: scale\n"
     ]
    }
   ],
   "source": [
    "image_bin = pp.segmentation.threshold(image, df_masks=masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting array `image_bin` is a binary image where white regions are foreground and black background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(image_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but there is still a lot of noise inside the image (isopod fecal pellets) that we need to remove. Right now, all information about foreground and background is in \"pixel-space\", i.e. it's a drawn representation of the informative areas. To convert it to coordinate space, and in the later steps extract information from the raw image, we will use `find_contours` on this binary image. But setting the argument `min_area`, we can set a minimum size for the area that the contours should have - 100 pixels should be enough to exclude all fecal pellets. Again, we can supply the image meta DataFrame to concatenate existing information. Afterwards we can draw the contours onto `canvas` with `show_contours` - the detected contours are in green. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_contours = pp.segmentation.find_contours(image_bin, df=img_data, min_area=100) \n",
    "canvas = pp.visualization.show_contours(canvas, df_contours=df_contours)\n",
    "pp.show_image(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - 2nd attempt\n",
    "\n",
    "Now we have excluded all noise - but some isopods are not well deteced. Mostly the ones with lighter pigmenation, because the contrast they form agains the light background isn't strong enough. We can try a different threshold algorithm by setting the `method` argument: `\"adaptive\"` algorithms work particularly well with variable contrast levels and lighting. \n",
    "\n",
    "Additionally, instead of using the default gray channel (i.e. the average across all three colour channels), we can try to run the thresholding function on a single colour channel. The contrast and signal-to-noise-ratio can be different betweeen the channels. We can select a different channel using the `select_canvas` function - here I isolate all three colour channels, and supply them to `show_image` to show themm all at once, and to evaluate, which colour-channels is suited best: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- red channel\n",
      "- green channel\n",
      "- blue channel\n"
     ]
    }
   ],
   "source": [
    "r = pp.visualization.select_canvas(image, canvas=\"r\")\n",
    "g = pp.visualization.select_canvas(image, canvas=\"g\")\n",
    "b = pp.visualization.select_canvas(image, canvas=\"b\")\n",
    "pp.show_image([r,g,b], max_dim=500) ## max_dim=reduce window size to 500 pixels on any axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this suggests that the green colour channel will yield the best results: here, even the lighter pigmented isopods have a strong contrast against the tray. We can supply either `g` to `threshold`, or directly select this with the `channel` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- applying mask: mask1\n",
      "- applying mask: scale\n"
     ]
    }
   ],
   "source": [
    "image_bin = pp.segmentation.threshold(image, method=\"adaptive\", df_masks=masks, channel=\"green\")\n",
    "pp.show_image(image_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - 3rd attempt\n",
    "\n",
    "Obviously we need to to tune a few things here, waay to many things are being detected. Currently the `\"adaptive\"` algorithm is on default sensitivity (`blocksize=99`), which we can reduce a bit. Also, we can increase the value for the constant to be subtracted after the tresholding with `constant`. We will try `blocksize=49` and `constant=5`. Afterwards, we plot the detected contours back onto the canvas of the original image, not the green channel image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style=\"width:800px; text-align: left\">\n",
    "\n",
    "![Binarization](_assets/ex1_binarization.jpg)\n",
    "        \n",
    "**Figure 2** - Demonstration of blocksize (19, 99, 199 - left to right) and constant (1, 5 - top and bottom) parameters. Increasing blocksize leads to better structuring of the pixel level information contained in the image (i.e. larger \"blocks\" of connected pixels can be detected). This is computationally costly, so it will be slow for large images. Also, there is an optimal value beyond which detection performance will decrease. \n",
    "\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- applying mask: mask1\n",
      "- applying mask: scale\n"
     ]
    }
   ],
   "source": [
    "image_bin = pp.segmentation.threshold(image, method=\"adaptive\", blocksize=49, \n",
    "                                      constant=5, df_masks=masks, channel=\"green\")\n",
    "df_contours = pp.segmentation.find_contours(image_bin, df=img_data, min_area=100)\n",
    "canvas = pp.visualization.show_contours(image, df_contours=df_contours, line_width=1)\n",
    "pp.show_image(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation - final product\n",
    "\n",
    "That looks pretty good. The last thing we need to take care of are the appendages (we don't want to include those) and the gaps that sometimes form between the segments (we want to have that error to be consistent towards no gaps). Some blurring, and a morphological operation will do the trick (see [the OpenCV docs](https://docs.opencv.org/3.4.9/d9/d61/tutorial_py_morphological_ops.html) and the [resources section](resources.html#computer-vision) of the documentation for more info). Blurring will smooth the contour of the isopods, and a farly large `\"cross\"` shaped kernel will \"cut off\" the appendages and other long structures in the binary image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- applying mask: mask1\n",
      "- applying mask: scale\n",
      " - show mask: mask1.\n",
      " - show mask: scale.\n"
     ]
    }
   ],
   "source": [
    "image_blurred = pp.segmentation.blur(image, kernel_size = 15)\n",
    "image_bin = pp.segmentation.threshold(image_blurred, method=\"adaptive\", blocksize=49, \n",
    "                                      constant=5, df_masks=masks,  channel=\"green\")\n",
    "image_morph = pp.segmentation.morphology(image_bin, operation=\"open\", shape=\"cross\", \n",
    "                                         kernel_size = 9, iterations=2)\n",
    "\n",
    "contours = pp.segmentation.find_contours(image_morph, df=img_data, min_area=250)\n",
    "canvas = pp.visualization.show_masks(image, df_masks=masks, line_width=3)\n",
    "canvas = pp.visualization.show_contours(canvas, df_contours=contours, line_width=1)\n",
    "pp.show_image(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring colour\n",
    "\n",
    "Ok, this looks good - we now have a DataFrame with the contour-data of all 20 isopods, including their length (`diameter` of the enclosing circle), and the shape (`coords`). Using this information, we can finally extract the colour information from inside the contours. To do so we can supply the original image array along with the contour DataFrame to the `colour_intensity` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pigmentation = pp.measurement.colour_intensity(image, df_image_data=img_data, df_contours=contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "Done - we can now save the contours and the colour information to csv, as well as the canvas for quality control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- contours saved under ../_temp/output\\contours.csv (overwritten).\n",
      "- colours saved under ../_temp/output\\colours.csv (overwritten).\n",
      "- canvas saved under ../_temp/output\\canvas.jpg (overwritten).\n"
     ]
    }
   ],
   "source": [
    "pp.export.save_contours(contours, dirpath=r\"../_temp/output\")\n",
    "pp.export.save_colours(pigmentation, dirpath=r\"../_temp/output\")\n",
    "pp.export.save_canvas(canvas, dirpath=r\"../_temp/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phenopype as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"images/isopods.jpg\"\n",
    "\n",
    "ct = pp.load_image(filepath, cont=True) ## load image as container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.preprocessing.create_mask(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.preprocessing.create_scale(ct, template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.df_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.visualization.show_masks(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(ct.canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.segmentation.threshold(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.segmentation.find_contours(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.visualization.show_contours(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(ct.canvas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.reset()\n",
    "pp.segmentation.threshold(ct)\n",
    "pp.segmentation.find_contours(ct, min_area=100)\n",
    "pp.visualization.show_contours(ct)\n",
    "pp.visualization.show_masks(ct)\n",
    "pp.show_image(ct.canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.show_image(ct.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
