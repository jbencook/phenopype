{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Working with phenopype\n",
    "\n",
    "Analysis of scientific images can be an iterative process that may require frequent user input to preprocess images, adjust settings and evaluate the obtained results. In phenopype, users can start this process by identifying the appropriate functions and settings to analyse a series of images (i.e. which segmentation algorithms is to be used). For the actual analysis, users then can switch to a workflow that has higher throughput and is more reproducible. Phenopype offers three different types of workflows that are explained here:\n",
    "\n",
    "1. [**Prototyping**](#proto) - images are loaded as arrays and functions are applied one by one (low throughput, low reproducibility - for workflow-prototyping and evaluation).\n",
    "2. [**Low throughput**](#low) - images are loaded into phenopype containers, which are then supplied to a function stack (medium throughput, low reproducibility - for single pictures and very small datasets).\n",
    "3. [**High throughput**](#high) - images are loaded from a phenopype project directory tree, and analyzed with the pype method (high throughput, high reproducibility - standard workflow for medium and large datasets).\n",
    "\n",
    "For the first two workflow types, users write a phenopype function stack in python code to assemble a CV workflow from five core modules (preprocessing, segmentation, measurement, export, visualization - for a full list check the API reference). This is recommended for users who wish to familiarize themselves with the basic principles of computer vision and to explore the phenopype function library. Output from all intermediate steps is returned from the functions and can be evaluated, which makes these routines are also appropriate for prototyping and testing. For medium and large datasets users should work from a python directory structure in conjunction with the pype-method; phenopype's high througput workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, all three workflows a demonstrated by analyzing an image of a threespine stickleback (*Gasterosteus aculeatus*) stained with alizarin red. Traits of interest are bone-plate area and shape, and, within the detected plates, pixel intensities that denote bone-density.\n",
    "\n",
    "<center>\n",
    "<div style=\"width:600px; text-align: left\" >\n",
    "    \n",
    "![Phenopype workflow example](_assets/workflow_example_case.png)\n",
    "    \n",
    "**Fig. 1:** Workflow demonstration using a stained stickleback. The computer vision functions used to extract the trait of interest (bone-plate area, shape and pixel density) are the same in all cases, but workflows differ in the amount of code necessary and in reproducibility. \n",
    "    \n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prototyping worflow <a name=\"proto\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low throughput workflow starts with the path to an image that is stored on the hard drive. `load_image` imports the file as a three-channel [1] numpy array (*ndarray*), together with image meta data (file name, exposure, dimensions, etc.) as a pandas *DataFrame*. The array gets passed on to the `threshold` function, which will return a binary array of the same dimensions. This array needs to be passed on to the `find_contours` function, which will return a dictionary with the detected contours. This dictionary, together with the original array, can then be passed to the `colour_intensity` function. This function will collect the average color value from within the perimeter coordinates for each contour and return a pandas dataframe containing those values. Finally, the dataframe can be exported as a csv file with `save_colour`. By passing on the initially created meta-data, the function will automatically expand the provided columns of meta-info into the exported csv.  \n",
    "\n",
    "[1] to learn more about the basic of Computer Vision check the resources section of the phenopype documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phenopype prototyping workflow](_assets/workflow_proto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Fig. 2:</strong> Schematic of Phenopype's prototyping workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Invalid image path - cannot load image from str.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Invalid image path - cannot load image from str.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "import phenopype as pp\n",
    "\n",
    "filepath = r\"/images/stickleback_side.jpg\"\n",
    "\n",
    "## load image as array, supply image_data (DataFrame containing meta data)\n",
    "image, image_data = pp.load_image(filepath, df = True, meta=True)\n",
    "## draw mask\n",
    "image_masked, mask = pp.preprocessing.create_mask(image, tool=\"polygon\") \n",
    "## thresholding converts multichannel to binary image\n",
    "image_bin = pp.segmentation.threshold(image, method=\"adaptive\", \n",
    "                                      channel=\"red\", blocksize=199, \n",
    "                                      constant=5, masks=mask) \n",
    "## perform morphology operations on binarized image\n",
    "image_morph = pp.segmentation.morphology(image_bin, operation=\"close\", \n",
    "                                         shape=\"ellipse\", kernel_size=3, \n",
    "                                         iterations=3) \n",
    "## detect contours ony binary image\n",
    "contours = pp.segmentation.find_contours(image_morph, df=image_data, \n",
    "                                         retrieval=\"ext\", min_area=150) \n",
    "## draw detected contours onto canvas\n",
    "image_drawn = pp.visualization.show_contours(image, contours=contours,\n",
    "                                             df=image_data, df_contours=contours)  \n",
    "## export contours to csv\n",
    "pp.export.save_contours(contours, dirpath = r\"../_temp/output\")\n",
    "## show convas\n",
    "pp.show_image(image_drawn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Low throughput worflow <a name=\"low\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_image` function can also load an image into a phenopype container, which is a python class that incorporates loaded images, dataframes, detected contours, intermediate output, etc. so that they are available for inspection or storage at the end of the analysis. The advantage of using containers is that they don’t litter the global environment and namespace, while still containing all intermediate steps (e.g. binary masks or contour DataFrames). Containers can be used manually to analyze images, but typically they are used automatically within the pype-routine that is part of phenoype's high throughput workflow (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phenopype low throughput workflow](_assets/workflow_low.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- create mask\n"
     ]
    }
   ],
   "source": [
    "import phenopype as pp\n",
    "\n",
    "filepath = r\"../assets/images/stickleback_side.jpg\"\n",
    "\n",
    "## load image as a phenopype container which will include all images, dataframes, \n",
    "## detected contours and intermediate output\n",
    "container = pp.load_image(filepath, cont=True, meta=True) \n",
    "\n",
    "## afterwards, same as in the prototyping workflow, functions are applied \n",
    "## directly to the container\n",
    "pp.preprocessing.create_mask(container, tool=\"polygon\") \n",
    "pp.segmentation.threshold(container, method=\"adaptive\", channel=\"red\", \n",
    "                          blocksize=199, constant=5) # 3/4\n",
    "pp.segmentation.morphology(container, operation=\"close\", shape=\"ellipse\", \n",
    "                           kernel_size=3, iterations=3) # 5\n",
    "pp.segmentation.find_contours(container, retrieval=\"ext\", min_area=150) # 6\n",
    "pp.visualization.show_contours(container) # 6\n",
    "pp.export.save_contours(container, dirpath = r\"../_temp/output\")\n",
    "pp.show_image(container.canvas) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. High throughput worflow <a name=\"high\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pype routine is phenopype’s standard method to analyse medium and large image datasets, where a function stack is constructed with the human readable yaml syntax (serialization language - for more information see the resources section of the phenopype documentation). Users can execute the pype method on a filepath, an array, or a phenopype directory, which always will trigger three actions: phenpype will i) open the contained yaml configuration with the default OS text editor, ii) parse the contained functions and execute them in the sequence, and iii) open a Python-window showing the processed image. After one parsing iteration, users can evaluate the results and decide to modify the opened configuration file (e.g. either change function parameters or add new functions), and run the pype again, or to terminate the pype and save all results (iv)). The processed image, any extracted phenotypic information, as well as the modified config-file is stored inside the image directory. Together with the raw images, which may be either stored separately or within the directory tree, users can thereby provide the full image analysis pipeline to anyone who wishes to reproduce the obtained results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further information** \n",
    "\n",
    "For more detailed information on `pype` and it's default behavior, see below.\n",
    "\n",
    "For more information on how to use the `pype` in conjunction with phenopype projects please refer to the [Phenopype project tutorial](tutorial_3_managing_projects_1.ipynb).\n",
    "\n",
    "Check the examples, which include both low and highthroughput code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Phenopype high throughput workflow](_assets/workflow_high.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------+++ new pype iteration 2020:03:25 15:03:29 +++--------------\n",
      "\n",
      "\n",
      "AUTOLOAD\n",
      "- masks_demo.csv\n",
      "PREPROCESSING\n",
      "create_mask\n",
      "- mask with label mask1 already created (overwrite=False)\n",
      "SEGMENTATION\n",
      "blur\n",
      "threshold\n",
      "- applying mask: mask1\n",
      "morphology\n",
      "find_contours\n",
      "VISUALIZATION\n",
      "select_canvas\n",
      "show_contours\n",
      "show_masks\n",
      " - show mask: mask1.\n",
      "EXPORT\n",
      "save_contours\n",
      "- contours saved under ../_temp/output\\contours_demo.csv (overwritten).\n",
      "save_canvas\n",
      "- canvas saved under ../_temp/output\\canvas_demo.jpg (overwritten).\n",
      "AUTOSAVE\n",
      "save_masks\n",
      "- masks not saved - file already exists (overwrite=False).\n",
      "\n",
      "\n",
      "TERMINATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phenopype.main.pype at 0x18b78889d48>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phenopype as pp\n",
    "\n",
    "filepath = r\"images/stickleback_side.jpg\"\n",
    "\n",
    "pp.pype(image=filepath, # input - can be also an array or a phenopype directory \n",
    "        dirpath = r\"../_temp/output\", ## where output is stored\n",
    "        name=\"demo\", # name of the  pype routine, appended to all results-files \n",
    "        preset=\"demo1\" # template for the analysis - you can create your own!\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 yaml-syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. `pype`-behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
